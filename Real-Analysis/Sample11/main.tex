\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage[margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{indentfirst}

\title{Power Series Representations}
\author{Matthew Seguin}
\date{}

\begin{document}

\maketitle

\section*{6.4.6}
\begin{center}
    \doublespacing
    Let $f(x) =\frac{1}{x} -\frac{1}{x+1} +\frac{1}{x+2} -\frac{1}{x+3} + ... =\sum _{n=0}^{\infty}\frac{(-1)^n}{x + n}$.
    \\For any $x_0 > 0$ we have that $f(x_0) =\frac{1}{x_0} -\frac{1}{x_0 +1} +\frac{1}{x_0 +2} -\frac{1}{x_0 +3} + ... =\sum _{n=0}^{\infty}\frac{(-1)^n}{x_0 + n}$ is a series of real numbers.
    \\Furthermore since $x_0 + n$ is strictly increasing as $n$ increases so we get that $\frac{1}{x_0 + n}$ is strictly decreasing as $n$ increases.
    \\Therefore by the alternating series test $f(x_0)$ converges for arbitrary $x_0 > 0$ and therefore for all $x_0 > 0$.
    \\So $f(x)$ is defined pointwise for all $x > 0$.
    \\Since $f(x)$ converges for all $x > 0$ we can regroup terms in the series.
    \\$f(x) =\frac{1}{x} -\frac{1}{x+1} +\frac{1}{x+2} -\frac{1}{x+3} + ... = (\frac{1}{x} -\frac{1}{x+1}) +(\frac{1}{x+2} -\frac{1}{x+3}) + ... =\frac{1}{x (x + 1)} +\frac{1}{(x+2)(x+3)} +\frac{1}{(x+4)(x+5)} + ...$
    \\Let $f_n (x) =\frac{1}{(x+n)(x+n+1)} =\frac{1}{x^2 + (2n+1)x + n(n+1)}$ so that $f(x) =\sum _{n=0}^{\infty} f_n (x)$.
    \\Since $x > 0$ we get that $|f_n (x)| =|\frac{1}{x^2 + (2n+1)x + n(n+1)}| =\frac{1}{x^2 + (2n+1)x + n(n+1)} <\frac{1}{n(n+1)}$ for all $n\geq 1$.
    \\So let $M_n =\frac{1}{n(n+1)}$ for $n\geq 1$ then since $\sum _{n=1}^{\infty}\frac{1}{n(n+1)}$ converges we get by the Weierstrass M-test that $\sum _{n=1}^{\infty} f_n (x)$ converges uniformly.
    \\We have seen previously that $\frac{1}{x}$ is continuous on $(0,\infty )$ and clearly the same is true for $\frac{1}{x+n}$ as it is just a translation of $\frac{1}{x}$ that is still defined on $(0,\infty )$ for all $n\in\mathbb{N}$.
    \\So $f_n (x)$ is continuous on $(0,\infty )$ for all $n\in\mathbb{N}$ and $\sum _{n=1}^{\infty} f_n (x)$ converges uniformly so $\sum _{n=1}^{\infty} f_n (x)$ is continuous on $(0,\infty )$ since uniform convergence preserves continuity.
    \\Now we still need to take care of $n = 0$. So look at $f_0 (x) =\frac{1}{x(x+1)} = (\frac{1}{x})(\frac{1}{x+1})$.
    \\Again $\frac{1}{x}$ is continuous on $(0,\infty )$ and the same is true for $\frac{1}{x+1}$, so by the algebraic continuity theorem we have $f_0 (x) =\frac{1}{x(x+1)}$ is continuous on $(0,\infty )$.
    \\Therefore $f(x) =\sum _{n=0}^{\infty} f_n (x) = f_0 (x) +\sum _{n=1}^{\infty} f_n (x)$ is continuous on $(0,\infty )$ by the algebraic continuity theorem.
    \\Let $g_n (x) =\frac{(-1)^n}{x + n}$ for $n\geq 0$ so that $f(x) =\sum _{n=0}^{\infty} g_n (x)$.
    \\Now for $n\in\mathbb{N}$ and $x > 0$ look at $g_n '(x) =\frac{(-1)^{n+1}}{(x+n)^2}$. Then $|g_n '(x)| =|\frac{(-1)^{n+1}}{(x+n)^2}| =\frac{1}{(x+n)^2} =\frac{1}{x^2 + 2nx + n^2} <\frac{1}{n^2}$ for $n\geq 1$.
    \\So taking $M_n =\frac{1}{n^2}$ for $n\geq 1$ since $\sum _{n=1}^{\infty}\frac{1}{n^2}$ converges we get by the Weierstrass M-test that $\sum _{n=1}^{\infty} g_n '(x)$ converges uniformly.
    \\So $\sum _{n=1}^{\infty} g_n '(x)$ converges uniformly and we also know $\sum _{n=1}^{\infty} g_n (x) = f(x) -\frac{1}{x}$ converges for all $x\in (0,\infty )$ therefore $\sum _{n=1}^{\infty} g_n (x)$ is differentiable on $(0,\infty )$.
    \\Now again we need to take care of $n = 0$, so look at $g_0 (x) =\frac{1}{x}$. We have previously seen $\frac{1}{x}$ is differentiable on $(0,\infty )$.
    \\Therefore $f(x) =\sum _{n=0}^{\infty} g_n (x) = g_0 (x) +\sum _{n=1}^{\infty} g_n (x)$ is differentiable on $(0,\infty )$ by the algebraic differentiability theorem.
    \\So we have shown $f(x)$ is defined on $(0,\infty )$, and is continuous and differentiable on $(0,\infty )$ \qedsymbol
\end{center}


\newpage
\section*{6.5.5}

{\Large\textbf{a.}} Let $s\in (0, 1)$ then let $(x_n) = (ns^{n-1})$.
\begin{center}
    \doublespacing
    Since $s > 0$ clearly $x_n > 0$ for all $n\in\mathbb{N}$.
    \\So all we need to show is that eventually $(x_n)$ is decreasing because this would mean all but finitely many points are decreasing and so we can bound $(x_n)$ by the maximum of those finitely many points.
    \\Let $(y_n) = (x_{n+1} - x_n) = ((n+1)s^n - ns^{n-1}) = (s^{n-1} (n(s - 1) + s))$.
    \\Let $N >\frac{s}{1-s}$, then for $n\geq N$ we have $n\geq N >\frac{s}{1-s}$ so $n(1-s) > s$ and $0 > s - n(1-s) = s + n(s-1)$ and so $0 > s^{n-1}(s + n(s-1))$ for all $n\geq N$ since $s > 0$.
    \\So we have that $y_n = x_{n+1} - x_n < 0$ and hence $x_{n+1} < x_n$ so $(x_n)$ is decreasing after $x_N$.
    \\Again let $M = max\{x_1, x_2, ..., x_N\}$ then we have that $M\geq x_n$ for all $n\in\mathbb{N}$ and since $0 < x_n$ for all $n\in\mathbb{N}$ we have that $|x_n|\leq M$ for all $n\in\mathbb{N}$ hence the sequence $(x_n) = (ns^{n-1})$ is bounded \qedsymbol
\end{center}

{\Large\textbf{b.}} Assume that $\sum _{n=0}^{\infty} a_n x^n$ converges for all $x\in (-R, R)$. Let $x\in (-R, R)$ then let $|x| < t < R$.
\begin{center}
    \doublespacing
    Then if $\sum _{n=0}^{\infty} |n a_n x^{n-1}|$ converges we know that $\sum _{n=0}^{\infty} n a_n x^{n-1}$ converges.
    \\So $\sum _{n=0}^{\infty} |n a_n x^{n-1}| = \sum _{n=0}^{\infty} n |a_n| |x|^{n-1} |\frac{t^n}{t^n}| =\sum _{n=0}^{\infty} \frac{n}{t} |a_n t^n| |\frac{x}{t}|^{n-1} =\sum _{n=0}^{\infty} \frac{1}{t} |a_n t^n| (n|\frac{x}{t}|^{n-1})$.
    \\Since $|x| < t$ we have that $|\frac{x}{t}| =\frac{|x|}{t} < 1$ and hence by part a we can let $M > 0$ be such that $n|\frac{x}{t}|^{n-1}\leq M$ for all $n\in\mathbb{N}$.
    \\So we have that $\sum _{n=0}^{\infty} |n a_n x^{n-1}| =\sum _{n=0}^{\infty} \frac{1}{t} |a_n t^n| (n|\frac{x}{t}|^{n-1})\leq\sum _{n=0}^{\infty} \frac{M}{t} |a_n t^n| =\frac{M}{t}\sum _{n=0}^{\infty} |a_n t^n|$.
    \\Since $t\in (|x|, R)$ there exists some $r\in (-R, R)$ satisfying $t < r < R$.
    \\Since $r\in (-R, R)$ we know $\sum _{n=0}^{\infty} a_n r^n$ converges and hence $\sum _{n=0}^{\infty} a_n t^n$ converges absolutely since $|t| < |r|$.
    \\Therefore $\sum _{n=0}^{\infty} |a_n t^n|$ converges and so $\frac{M}{t}\sum _{n=0}^{\infty} |a_n t^n|$ converges so $\sum _{n=0}^{\infty} |n a_n x^{n-1}|$ converges.
    \\Since $\sum _{n=0}^{\infty} |n a_n x^{n-1}|$ converges this means $\sum _{n=0}^{\infty} n a_n x^{n-1}$ converges.
    \\This was for arbitrary $x\in (-R, R)$ and is therefore true for all $x\in (-R, R)$.
    \\So if $\sum _{n=0}^{\infty} a_n x^n$ converges for all $x\in (-R, R)$ then so does $\sum _{n=0}^{\infty} n a_n x^{n-1}$ \qedsymbol
\end{center}


\newpage
\section*{6.5.7}
\begin{center}
    Let $\sum a_n x^n$ be a power series where $a_n\neq 0$ and assume $L = lim_{n\rightarrow\infty} |\frac{a_{n+1}}{a_n}|$ exists.
\end{center}

{\Large\textbf{a.}} Assume that $L\neq 0$, then let $x\in (-\frac{1}{L},\frac{1}{L})$. Then let $y_n = a_n x^n$.
\begin{center}
    \doublespacing
    If $x = 0$ then $y_n = 0$ for all $n\in\mathbb{N}$ and clearly then $\sum y_n =\sum a_n x^n$ converges.
    \\Otherwise consider $|\frac{y_{n+1}}{y_n}| = |\frac{a_{n+1} x^{n+1}}{a_n x^n}| = |\frac{a_{n+1} x}{a_n}| = |x||\frac{a_{n+1}}{a_n}|$. Note that here $|x|$ is a fixed constant.
    \\So by the algebraic limit theorem $lim_{n\rightarrow\infty} |\frac{y_{n+1}}{y_n}| = lim_{n\rightarrow\infty} |x||\frac{a_{n+1}}{a_n}| = |x|\;lim_{n\rightarrow\infty} |\frac{a_{n+1}}{a_n}| = |x| L <\frac{1}{L} L = 1$.
    \\So by the ratio test if $L\neq 0$ and $x\in (-\frac{1}{L},\frac{1}{L})$ then $\sum a_n x^n$ converges \qedsymbol
\end{center}

{\Large\textbf{b.}} Assume that $L = 0$, then let $x\in\mathbb{R}$. Then let $y_n = a_n x^n$.
\begin{center}
    \doublespacing
    If $x = 0$ then $y_n = 0$ for all $n\in\mathbb{N}$ and clearly then $\sum y_n =\sum a_n x^n$ converges.
    \\Otherwise consider $|\frac{y_{n+1}}{y_n}| = |\frac{a_{n+1} x^{n+1}}{a_n x^n}| = |\frac{a_{n+1} x}{a_n}| = |x||\frac{a_{n+1}}{a_n}|$. Note that here $|x|$ is a fixed constant.
    \\So by the algebraic limit theorem $lim_{n\rightarrow\infty} |\frac{y_{n+1}}{y_n}| = lim_{n\rightarrow\infty} |x||\frac{a_{n+1}}{a_n}| = |x|\;lim_{n\rightarrow\infty} |\frac{a_{n+1}}{a_n}| = 0 < 1$.
    \\So by the ratio test if $L = 0$ and $x\in\mathbb{R}$ then $\sum a_n x^n$ converges \qedsymbol
\end{center}

{\Large\textbf{c.}} Now let $L' = lim_{n\rightarrow\infty} s_n$ where $s_n = sup\{|\frac{a_{k+1}}{a_k}| : k\geq n\}$.
\begin{center}
    \doublespacing
    First I will prove that for a sequence $(b_n)$ if $lim_{n\rightarrow\infty} t_n = M < 1$ then $\sum b_n$ converges where $t_n = sup\{|\frac{b_{k+1}}{b_k}| : k\geq n\}$.
    \\Choose $y\in (M, 1)$ such a $y$ exists because $M < 1$.
    \\Clearly $(t_n)$ is decreasing as if $n$ increases then the supremum is of a subset of the original one and is therefore less than or equal to the supremum before.
    \\Since $(t_n)\rightarrow M < y$ and $(t_n)$ is decreasing there must exist some $N\in\mathbb{N}$ such that $|\frac{b_{n+1}}{b_n}| < y$ for all $n\geq N$.
    \\So $|b_{n+1}| < y|b_n|$ for all $n\geq N$. I will show by induction that $|b_n| < y^{n - N} |b_N|$ for all $n > N$.
    \\Let $S =\{n\in\mathbb{N} : |b_n| < y^{n - N} |b_N|\}$.
    \\For our base case we know that for $n = N + 1$ we have $|b_n| = |b_{N+1}| < y|b_N| = y^{n-N} |b_N|$. So $N+1\in S$.
    \\Assume that $n\in S$, then $|b_n| < y^{n-N} |b_N|$ so $|b_{n+1}| < y |b_n| < y (y^{n-N} |b_N|) = y^{(n+1) - N} |b_N|$ and $n+1\in S$.
    \\Therefore $n\in S$ for all $n > N$ by induction and hence $|b_n| < y^{n - N} |b_N|$ for all $n > N$.
    \\So $\sum _{n = N+1}^{\infty} |b_n| <\sum _{n = N+1}^{\infty} y^{n-N} |b_N| = |b_N|\sum _{k=1}^{\infty} y^k$ which converges since $y < 1$ and this is a geometric series.
    \\By the comparison test $\sum _{n = N+1}^{\infty} |b_n|$ converges and therefore $\sum |b_n| =\sum _{n\leq N} |b_n| +\sum _{n = N+1}^{\infty} |b_n|$ converges since the first sum is finite. So $\sum b_n$ converges and we are done with this proof.
    \\Now for our example let $t_n = sup\{|\frac{a_{k+1} x^{k+1}}{a_k x^k}| : k\geq n\}$. Assume that $L'\neq 0$ and that $x\in (-\frac{1}{L'},\frac{1}{L'})$. Then let $y_n = a_n x^n$
    \\Again if $x = 0$ then clearly $\sum a_n x^n$ converges.
    \\Otherwise consider $|\frac{y_{n+1}}{y_n}| = |\frac{a_{n+1} x^{n+1}}{a_n x^n}| = |x||\frac{a_{n+1}}{a_n}|$, then clearly $lim_{n\rightarrow\infty} t_n = |x| lim_{n\rightarrow\infty} s_n = |x| L' <\frac{1}{L'} L' = 1$.
    \\So by the proof before we know that $\sum a_n x^n$ converges.
    \\Now assume that $L' = 0$ and let $x\in\mathbb{R}$. Then let $y_n = a_n x^n$.
    \\Again if $x = 0$ then clearly $\sum a_n x^n$ converges.
     \\Otherwise consider $|\frac{y_{n+1}}{y_n}| = |\frac{a_{n+1} x^{n+1}}{a_n x^n}| = |x||\frac{a_{n+1}}{a_n}|$, then clearly $lim_{n\rightarrow\infty} t_n = |x| lim_{n\rightarrow\infty} s_n = 0 < 1$.
     \\So by the proof before we know that $\sum a_n x^n$ converges.
     \\Therefore the result still holds if $L$ is replaced by $L' = lim_{n\rightarrow\infty} s_n$ \qedsymbol
\end{center}


\newpage
\section*{6.6.6}
\begin{center}
    Let $g(0) = 0$ and $g(x) = e^{-\frac{1}{x^2}}$ for $x\neq 0$.
\end{center}

{\Large\textbf{a.}} We are given that $g'(0) = 0$.
\begin{center}
    \doublespacing
    Let $c\neq 0$ then $g'(c) = (e^{-\frac{1}{c^2}})(\frac{2}{c^3})$ by using the chain rule and the fact that $e^y$ is its own derivative.
    \\Now let's find $g''(0) = lim_{x\rightarrow 0}\frac{g'(x) - g'(0)}{x - 0} = lim_{x\rightarrow 0}\frac{\frac{2}{x^3} e^{-\frac{1}{x^2}}}{x} = lim_{x\rightarrow 0}\frac{2 e^{-\frac{1}{x^2}}}{x^4}$ which satisfies the 0/0 case for L'hospital's rule because $\frac{1}{x^2}$ grows arbitrarily large and hence $e^{-\frac{1}{x^2}}$ grows arbitrarily close to 0.
    \\So $g''(0) = lim_{x\rightarrow 0}\frac{2 e^{-\frac{1}{x^2}}}{x^4} = lim_{x\rightarrow 0}\frac{2}{x^4 e^{\frac{1}{x^2}}} = lim_{x\rightarrow 0}\frac{0}{4 x^3 e^{\frac{1}{x^2}} + x^4 e^{\frac{1}{x^2}} (-\frac{2}{x^3})} = 0$.
\end{center}

{\Large\textbf{b.}} We have from before that $g'(x) =\frac{2}{x^3} e^{-\frac{1}{x^2}}$ for $x\neq 0$.
\begin{center}
    \doublespacing
    So for $x\neq 0$ we have $g''(x) = -\frac{6}{x^4} e^{-\frac{1}{x^2}} + (\frac{2}{x^3})(\frac{2}{x^3}) e^{-\frac{1}{x^2}} = (\frac{4}{x^6} -\frac{6}{x^4}) e^{-\frac{1}{x^2}}$.
    \\And thus for $x\neq 0$ we have $g'''(x) = (-\frac{24}{x^7} +\frac{24}{x^5}) e^{-\frac{1}{x^2}} + (\frac{4}{x^6} -\frac{6}{x^4})(\frac{2}{x^3}) e^{-\frac{1}{x^2}} = (\frac{8}{x^9} -\frac{36}{x^7} +\frac{24}{x^5}) e^{-\frac{1}{x^2}}$.
    \\I claim that $f^{(n)} (x)$ is of the form $(\sum _{k=1}^{n} a_k x^{-(n + 2k)}) e^{-\frac{1}{x^2}}$ for all $n\in\mathbb{N}$ when $x\neq 0$ and I will show this by induction.
    \\Let $S =\{n\in\mathbb{N} : f^{(n)} (x) = (\sum _{k=1}^{n} a_k x^{-(n + 2k)}) e^{-\frac{1}{x^2}}\}$.
    \\For our base case we know $g'(x) = g^{(1)} (x) = (2x^{-3})e^{-\frac{1}{x^2}} = (a_1 x^{-(1 + 2(1))})e^{-\frac{1}{x^2}}$ so $1\in S$.
    \\Now assume $n\in S$, that is assume $f^{(n)} (x) = (\sum _{k=1}^{n} a_k x^{-(n + 2k)}) e^{-\frac{1}{x^2}}$.
    \\Then $f^{(n+1)} (x) = (\sum _{k=1}^n (a_k)(-(n+2k))x^{-(n+2k+1)})e^{-\frac{1}{x^2}} + (\sum _{k=1}^{n} a_k x^{-(n + 2k)})(\frac{2}{x^3})e^{-\frac{1}{x^2}} = ((\sum _{k=1}^n b_k x^{-((n+1)+2k)}) + (\sum _{k=1}^n c_k x^{-((n+1)+2(k+1))}))e^{-\frac{1}{x^2}} = (\sum _{k=1}^{n+1} d_k x^{-((n+1) + 2k)})e^{-\frac{1}{x^2}}$. Therefore $n+1\in S$.
    \\So by induction $S =\mathbb{N}$ and hence $f^{(n)} (x) = (\sum _{k=1}^{n} a_k x^{-(n + 2k)}) e^{-\frac{1}{x^2}}$ for all $n\in\mathbb{N}$ when $x\neq 0$.
\end{center}

{\Large\textbf{c.}} Let $S =\{n\in\mathbb{N} : g^{(n)} (0) = 0\}$. We already know $g'(0) = g^{(1)} (0) = 0$, so $1\in S$.
\begin{center}
    \doublespacing
    Now assume $n\in S$, that is $g^{(n)} (0) = 0$.
    \\Then $g^{(n+1)} (0) = lim_{x\rightarrow 0}\frac{g^{(n)} (x) - g^{(n)} (0)}{x - 0} = lim_{x\rightarrow 0}\frac{(\sum _{k=1}^{n} a_k x^{-(n + 2k)}) e^{-\frac{1}{x^2}}}{x} = lim_{x\rightarrow 0}\frac{e^{-\frac{1}{x^2}}}{(\sum _{k=1}^{n} a_k x^{(n + 2k)})x}$ satisfies the 0/0 case for L'hospital's rule.
    \\So $g^{(n+1)} (0) = lim_{x\rightarrow 0}\frac{e^{-\frac{1}{x^2}}}{(\sum _{k=1}^{n} a_k x^{(n + 2k)})x} = lim_{x\rightarrow 0}\frac{1}{(\sum _{k=1}^{n} a_k x^{(n + 2k+1)}) e^{\frac{1}{x^2}}} = lim_{x\rightarrow 0}\frac{0}{(\sum _{k=1}^{n} b_k x^{(n + 2k + 2)}) e^{\frac{1}{x^2}} + (\sum _{k=1}^{n} b_k x^{(n + 2k + 1)})(\frac{2}{x^3}) e^{\frac{1}{x^2}}} = 0$
    \\Therefore $n+1\in S$. So by induction $S =\mathbb{N}$ and hence $g^{(n)} (0) = 0$ for all $n\in\mathbb{N}$ \qedsymbol
\end{center}


\newpage
\section*{6.3.7}
\begin{center}
    \doublespacing
    Let $(f_n)$ be a sequence of differentiable functions on $[a, b]$ such that $(f_n')$ converges uniformly and at some point $x_0\in [a, b]$, $(f_n (x_0))$ is convergent.
    \\Let $\epsilon > 0$ and let $\alpha =\epsilon / 2$ and let $\beta =\frac{\epsilon}{2(b-a)}$.
    \\Then since $(f_n (x_0))$ is convergent it is also Cauchy so let $N_1\in\mathbb{N}$ be such that for all $m, n\geq N_1$ we have $|f_n (x_0) - f_m (x_0)| <\alpha$.
    \break
    \\Furthermore we know each $f_n$ is differentiable so for all $m, n\in\mathbb{N}$ we have $(f_n - f_m)' = f_n' - f_m'$.
    \\Since $(f_n')\rightarrow g$ uniformly for some $g$ we have that $(f_n')$ satisfies the Cauchy criterion for uniform convergence.
    \\So let $N_2$ be such that $|f_n'(x) - f_m'(x)| <\beta$ when $m, n\geq N_2$ and $x\in [a, b]$.
    \\Then for any $x\in [a, b]$ we can apply the mean value theorem to $f_n - f_m$ on $[x, x_o]$ to conclude there exists a $c\in [x, x_0]$ such that $|(f_n (x) - f_m(x)) - (f_n (x_0) - f_m (x_0))| = |x - x_0||f_n' (c) - f_m' (c)|$.
    \\Therefore for $m, n\geq N_2$ we have $|(f_n (x) - f_m(x)) - (f_n (x_0) - f_m (x_0))| = |x - x_0||f_n' (c) - f_m' (c)|\leq (b - a)|f_n'(c) - f_m'(c)| < (b-a)\beta = (b-a)\frac{\epsilon}{2(b-a)} =\epsilon / 2 =\alpha$.
    \break
    \\Let $N = max\{N_1, N_2\}$ this $N$ exists since we are looking at a finite set.
    \\Then for $m, n\geq N$ we have that $|f_n (x_0) - f_m (x_0)| <\alpha$ and $|(f_n (x) - f_m(x)) - (f_n (x_0) - f_m (x_0))| <\alpha$.
    \\Therefore for $m, n\geq N$ we have $|f_n (x) - f_m (x)| = |f_n (x) - f_m (x) - (f_n (x_0) - f_m (x_0)) + (f_n (x_0) - f_m (x_0))|\leq |(f_n (x) - f_m(x)) - (f_n (x_0) - f_m (x_0))| + |f_n (x_0) - f_m (x_0)| < 2\alpha =\epsilon$.
    \\So for all $\epsilon > 0$ we have found an $N\in\mathbb{N}$ such that $|f_n (x) - f_m (x)| <\epsilon$ when $m, n\geq N$ and $x\in [a, b]$.
    \\Therefore $(f_n)$ converges uniformly on $[a, b]$ \qedsymbol
\end{center}

\end{document}
